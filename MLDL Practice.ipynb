{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca542e59-614d-4f2f-8ae6-f8b149851292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# 여러 열에 대해 서로 다른 전처리기를 적용할 수 있게 함 \n",
    "# 숫자형 열에는 표준화 _ StandardScaler\n",
    "# 범주형 열에는 _ OneHotEncoder \n",
    "# 이런 걸 한 번에 처리할 수 있음 \n",
    "from sklearn.pipeline import Pipeline\n",
    "# 여러 단계의 머신러닝 작업을 하나로 묶어서 깔끔 처리하게 해주는 도구 \n",
    "# pipe = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),      # 결측값 처리\n",
    "#     ('scaler', StandardScaler()),                     # 정규화\n",
    "#     ('classifier', LogisticRegression())              # 모델 학습\n",
    "# ])\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "# 결측값 처리 위한 도구 불러오는 코드 \n",
    "# 데이터셋에 **NaN(결측값)**이 있을 때, 그 자리를 평균, 중앙값, 최빈값 등으로 채워주는 역할\n",
    "# - strategy='mean': 평균값으로 채움\n",
    "# - strategy='median': 중앙값으로 채움\n",
    "# - strategy='most_frequent': 가장 많이 나온 값으로 채움\n",
    "# - strategy='constant': 사용자가 지정한 값으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a909fb85-0e93-4f3a-abe6-831c8245d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "# rc: runtime configuration\n",
    "rc(\"font\", family='Malgun Gothic')\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c34a323-df2c-4a61-a28d-365ffa0ec6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "submission = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994eac5e-0f2d-4a5a-8d3c-049d33cc607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>7</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>25</td>\n",
       "      <td>aug</td>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>514</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>18</td>\n",
       "      <td>jun</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>602</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14</td>\n",
       "      <td>may</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>28</td>\n",
       "      <td>may</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>889</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>feb</td>\n",
       "      <td>902</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age          job  marital  education default  balance housing loan  \\\n",
       "0   0   42   technician  married  secondary      no        7      no   no   \n",
       "1   1   38  blue-collar  married  secondary      no      514      no   no   \n",
       "2   2   36  blue-collar  married  secondary      no      602     yes   no   \n",
       "3   3   27      student   single  secondary      no       34     yes   no   \n",
       "4   4   26   technician  married  secondary      no      889     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
       "0  cellular   25   aug       117         3     -1         0  unknown  0  \n",
       "1   unknown   18   jun       185         1     -1         0  unknown  0  \n",
       "2   unknown   14   may       111         2     -1         0  unknown  0  \n",
       "3   unknown   28   may        10         2     -1         0  unknown  0  \n",
       "4  cellular    3   feb       902         1     -1         0  unknown  1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cc42bf-34b9-46d6-b041-2633b95e41e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>32</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1397</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>21</td>\n",
       "      <td>may</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750001</td>\n",
       "      <td>44</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>23</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>apr</td>\n",
       "      <td>586</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750002</td>\n",
       "      <td>36</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>46</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>13</td>\n",
       "      <td>may</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750003</td>\n",
       "      <td>58</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-1380</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29</td>\n",
       "      <td>may</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750004</td>\n",
       "      <td>28</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1950</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>22</td>\n",
       "      <td>jul</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  age            job  marital  education default  balance housing  \\\n",
       "0  750000   32    blue-collar  married  secondary      no     1397     yes   \n",
       "1  750001   44     management  married   tertiary      no       23     yes   \n",
       "2  750002   36  self-employed  married    primary      no       46     yes   \n",
       "3  750003   58    blue-collar  married  secondary      no    -1380     yes   \n",
       "4  750004   28     technician   single  secondary      no     1950     yes   \n",
       "\n",
       "  loan   contact  day month  duration  campaign  pdays  previous poutcome  \n",
       "0   no   unknown   21   may       224         1     -1         0  unknown  \n",
       "1   no  cellular    3   apr       586         2     -1         0  unknown  \n",
       "2  yes  cellular   13   may       111         2     -1         0  unknown  \n",
       "3  yes   unknown   29   may       125         1     -1         0  unknown  \n",
       "4   no  cellular   22   jul       181         1     -1         0  unknown  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d788179-c437-4de5-a169-94339462bf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_in_train = set(train.columns) - set(test.columns)\n",
    "unique_in_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e088ca4-959c-4353-8c8c-0fb3e05e8f79",
   "metadata": {},
   "source": [
    "# 입력 변수(X), 타겟(y) 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bcdbb93-f3dd-45e7-ae20-e7653603fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"y\"])\n",
    "y = train[\"y\"]\n",
    "\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104b64c-02ca-4440-81c8-1ad38e59421c",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a5e4b9-a083-4474-8c2d-c3e0b06ff9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hto encoding / scailing은 train + test 합쳐서 fit 하고 다시 나눠주는 게 안전 \n",
    "full = pd.concat([X, X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a33a019-caf5-4669-bb1b-3cab05568d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'age', 'job', 'marital', 'education', 'default', 'balance',\n",
       "       'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign',\n",
       "       'pdays', 'previous', 'poutcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db16fec-3b75-41bf-b2cc-d25c02f6d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            int64\n",
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default      object\n",
       "balance       int64\n",
       "housing      object\n",
       "loan         object\n",
       "contact      object\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf27ff6-8e8b-4a9b-a45f-1144417c1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   id         750000 non-null  int64 \n",
      " 1   age        750000 non-null  int64 \n",
      " 2   job        750000 non-null  object\n",
      " 3   marital    750000 non-null  object\n",
      " 4   education  750000 non-null  object\n",
      " 5   default    750000 non-null  object\n",
      " 6   balance    750000 non-null  int64 \n",
      " 7   housing    750000 non-null  object\n",
      " 8   loan       750000 non-null  object\n",
      " 9   contact    750000 non-null  object\n",
      " 10  day        750000 non-null  int64 \n",
      " 11  month      750000 non-null  object\n",
      " 12  duration   750000 non-null  int64 \n",
      " 13  campaign   750000 non-null  int64 \n",
      " 14  pdays      750000 non-null  int64 \n",
      " 15  previous   750000 non-null  int64 \n",
      " 16  poutcome   750000 non-null  object\n",
      "dtypes: int64(8), object(9)\n",
      "memory usage: 97.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d72b7b8-806d-41a7-a2c8-aca13432e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = full.select_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a367838b-4de8-4b95-8ca6-d846c12d9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = full.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "# OneHotEncoding\n",
    "full = pd.get_dummies(full, columns=cat_cols, drop_first=True)\n",
    "# Scailing \n",
    "# scaler = StandardScaler()\n",
    "# full[num_cols] = scaler.fit_transform(full[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25c5596-e738-402e-9b57-b7f62f340257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'poutcome']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e3a96c-e6b2-4ad8-8502-0e22061f3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full.iloc[:len(X), :]\n",
    "X_test = full.iloc[len(X):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3635e585-50bf-42a8-afb0-52646767226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "# stratify=y: 데이터 나눌 때 비율 유지하겠다는 뜻 _ 각 클래스(0,1)의 비율이 전체 데이터의 y와 같게 나누어짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6973cd8a-9c3c-426e-bf71-79b1fb4cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.replace(\"unknown\", np.nan)\n",
    "X_valu = X_val.replace(\"unknown\", np.nan)\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"sc\", StandardScaler())])\n",
    "# pipeline 안에 여러 단계가 있을 때, 각각을 식별하는 용도로 imp, sc를 씀 \n",
    "# 예를 들어 GridSearchCV를 사용할 때, 각 단계의 파라미터를 조정하려면 이름이 있어야 함 \n",
    "# param_grid = {\n",
    "#     'imp__strategy': ['mean', 'median'],\n",
    "#     'sc__with_mean': [True, False]\n",
    "# }\n",
    "cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33380e2d-fc0d-4d79-8d5c-ce587188a9b6",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acca7b-66d6-411e-8b19-73be1d2e06ba",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba56b0-8c54-48bc-939f-b0649f4c5001",
   "metadata": {},
   "source": [
    "## kernel\n",
    "- 복잡한 패턴을 잡으려면 입력을 고차원 특징공간으로 바꿔서 선형으로 자르면 쉽다(“특징 맵” φ(x)).\n",
    "- 그런데 φ(x)가 엄청 고차원이면 직접 계산이 너무 비싸다.\n",
    "- 커널 트릭: 두 점의 고차원 내적 ⟨φ(x), φ(z)⟩을 커널 함수 K(x, z)로 바로 계산한다.\n",
    "- 즉, 맵핑은 “암시적으로” 하고, 우리는 K만 계산하면 된다.\n",
    "- SVM/커널릿들이 결국 “커널 행렬(Gram matrix)” K_ij = K(x_i, x_j)만으로 학습한다.\n",
    "### RBF(가우시안) 커널\n",
    "정의: K(x,z)=exp(−γ∥x−z∥2)\\\n",
    "또는 γ=1/2σ^2로 쓰기도 함.\\\n",
    "해석: 두 샘플이 가까우면(유클리드 거리 작으면) 유사도≈1, 멀면 0에 수렴.\\\n",
    "즉, **“가까운 이웃은 비슷하다”**는 가정으로 매끈한 곡면 결정경계를 만든다.\\\n",
    "특징: 무한차원 특징공간에 해당(이론적으로 매우 표현력 풍부). 그래서 범용으로 잘 먹힌다.\n",
    "\n",
    "**핵심 하이퍼파라미터**\n",
    "- γ (gamma): “가까움”의 기준(스케일).\n",
    "    - 작은 γ → 넓은 종(bandwidth 큼) → 부드러운 경계(바이어스↑, 분산↓).\\\n",
    "    - 큰 γ → 좁은 종(bandwidth 작음) → 요철 많은 경계(바이어스↓, 분산↑, 과적합 위험).\\\n",
    "    - 관계: σ=1/2γ\\\n",
    "    - scikit-learn 기본값 gamma=\"scale\"은 \\gamma = \\frac{1}{\\text{n_features} \\cdot \\text{Var}(X)}.\n",
    "- C(규제 강도): 마진 최대화 vs 오분류 패널티의 트레이드오프.\n",
    "    - 큰 C: 오분류를 덜 허용 → 경계가 복잡해질 수 있음(과적합 위험).\n",
    "    - 작은 C: 오분류 좀 허용 → 경계가 부드러움(과소적합 위험).\n",
    "- C와 γ의 상호작용:\n",
    "    - 둘 다 복잡도를 키우는 방향으로 작동 가능 → **(C↑, γ↑)**는 가장 공격적(과적합 주의).\n",
    "    - 보통 로그스케일 그리드 서치로 함께 튜닝한다.\n",
    "\n",
    "**꼭 지켜야 하는 전처리**\n",
    "- 스케일링 필수(표준화/MinMax). 거리 기반이므로 단위·스케일이 다르면 왜곡됨.\n",
    "- 원-핫 고차원 희소벡터가 너무 많으면 거리의 의미가 약해질 수 있음 → 트리계열이나 선형 모델 검토.\n",
    "\n",
    "**언제 RBF가 좋은가?**\n",
    "- 중/소규모 표본에서 비선형 경계가 필요한 분류/회귀.\n",
    "- 특징 수는 적당하고(수~수십), 스케일링이 잘 되어 있을 때.\n",
    "\n",
    "**한계와 대안**\n",
    "- 시간/메모리 복잡도가 샘플 수에 비선형으로 커진다(커널 행렬 O(n²)).\\\n",
    "데이터가 아주 크면 Linear SVM, XGBoost/LightGBM, 혹은 근사 커널(Nystrom, RFF) 고려.\n",
    "- 확률 출력이 필요하면 probability=True로 Platt scaling(추가 비용)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c08ee-e322-4909-9511-19d49bc48c53",
   "metadata": {},
   "source": [
    "# 🌲 Step-by-Step: 랜덤 포레스트의 작동 방식\n",
    "\n",
    "1. 부트스트랩 샘플링 (Bagging)\n",
    "- 전체 이메일 데이터에서 중복 허용하며 랜덤하게 샘플을 뽑아 여러 개의 작은 데이터셋을 만든다.\n",
    "- 예를 들어, 400개의 트리를 만들기로 했으니, 400개의 서로 다른 샘플링된 데이터셋이 생긴다.\n",
    "  \n",
    "2. 랜덤 피처 선택\n",
    "- 각 트리는 학습할 때 전체 피처 중 일부만 랜덤하게 선택해서 사용한다.\n",
    "- 예를 들어, 어떤 트리는 \"무료\"와 \"첨부파일\"만 보고 판단하고, 다른 트리는 \"발신자\"와 \"이메일 길이\"만 본다.\n",
    "\n",
    "3. 개별 결정 트리 학습\n",
    "- 각 트리는 자신만의 데이터와 피처를 가지고 스팸인지 아닌지를 판단하는 규칙을 만든다.\n",
    "- 어떤 트리는 \"무료\"라는 단어가 있으면 스팸이라고 하고, 다른 트리는 \"첨부파일\"이 있으면 스팸이라고 할 수도 있어.\n",
    "  \n",
    "4. 예측 시 투표\n",
    "- 새로운 이메일이 들어오면, 400개의 트리 각각이 자신의 기준으로 판단한다.\n",
    "- 예: 400개 중 320개가 \"스팸\"이라고 하면 → 최종 예측은 \"스팸\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c66fc-ed98-4060-b78d-ca697e4cc12f",
   "metadata": {},
   "source": [
    "# 🌱 GBDT의 작동 원리: “잔차를 줄여가며 트리를 쌓는다”\n",
    "\n",
    "1. 첫 번째 트리\n",
    "- 처음엔 아주 단순한 트리를 하나 만든다.\n",
    "- 이 트리는 대충 평균값이나 간단한 규칙으로 예측을 한다.\n",
    "- 당연히 오차(잔차)가 많이 생기겠지?\n",
    "  \n",
    "2. 두 번째 트리\n",
    "- 이제 첫 번째 트리가 틀린 부분(잔차)을 학습해서 보정하는 트리를 만든다.\n",
    "- 예를 들어, 첫 번째 트리가 5억이라고 예측했는데 실제는 6억이면, 두 번째 트리는 그 1억 차이를 줄이려고 노력함.\n",
    "\n",
    "3. 세 번째, 네 번째...\n",
    "- 이런 식으로 잔차를 줄이는 트리들을 계속 쌓아가면서, 점점 더 정교한 예측을 하게 돼.\n",
    "- 각 트리는 약한 모델이지만, 모두 합치면 강력한 모델이 되는 거야.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d73d14-13d3-45fb-a72c-df3cad4ed1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 XGBClassifier란?\n",
    "\n",
    "`XGBClassifier`는 **XGBoost (Extreme Gradient Boosting)** 라이브러리의 분류기 버전이야. 이름부터 “익스트림”이 붙었지? 그만큼 성능과 효율을 극한까지 끌어올린 GBDT의 업그레이드 버전이라고 보면 돼.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 GBDT vs XGBoost: 뭐가 달라졌을까?\n",
    "\n",
    "| 항목 | GradientBoostingClassifier | XGBClassifier |\n",
    "|------|-----------------------------|---------------|\n",
    "| **학습 방식** | 잔차 기반 부스팅 | 잔차 + 정규화 + 더 똑똑한 트리 |\n",
    "| **속도** | 느림 (순차적 학습) | 빠름 (병렬 처리, 캐시 최적화) |\n",
    "| **과적합 방지** | learning_rate로 조절 | 정규화(L1/L2), early stopping 등 다양함 |\n",
    "| **결측치 처리** | 직접 처리 필요 | 자동 처리 가능 |\n",
    "| **성능** | 좋음 | 더 좋음 (특히 대규모 데이터에서) |\n",
    "| **사용 편의성** | 비교적 단순 | 파라미터 많지만 튜닝 여지 큼 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 XGBClassifier의 핵심 발전 포인트\n",
    "\n",
    "### 1. **정규화(Regularization)**\n",
    "- GBDT는 트리를 계속 쌓다 보면 과적합 위험이 커.\n",
    "- XGBoost는 **L1, L2 정규화**를 통해 모델 복잡도를 제어해서 과적합을 방지해.\n",
    "\n",
    "### 2. **병렬 처리**\n",
    "- GBDT는 트리를 하나씩 순차적으로 학습하지만,\n",
    "- XGBoost는 **노드 분할을 병렬로 처리**해서 훨씬 빠르게 학습함.\n",
    "\n",
    "### 3. **결측치 자동 처리**\n",
    "- XGBoost는 결측값이 있어도 알아서 처리해줘서 전처리 부담이 줄어들어.\n",
    "\n",
    "### 4. **Early Stopping**\n",
    "- 검증 성능이 더 이상 좋아지지 않으면 학습을 멈추는 기능.\n",
    "- 불필요한 트리 생성을 막고, 과적합도 줄여줌.\n",
    "\n",
    "### 5. **트리 구조 최적화**\n",
    "- 기존 GBDT는 깊이 우선 방식으로 트리를 만들지만,\n",
    "- XGBoost는 **최적 분할을 더 똑똑하게 계산**해서 성능이 더 좋아.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏡 예시로 다시 돌아가보자: 집값 예측\n",
    "\n",
    "- GBDT는 집값을 예측할 때, 방 개수나 평수 같은 피처를 기반으로 잔차를 줄여가며 트리를 쌓아.\n",
    "- XGBoost는 같은 작업을 하되, **더 빠르게**, **더 정교하게**, 그리고 **과적합을 막으면서** 예측해.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 한 줄 요약\n",
    "\n",
    "> XGBClassifier는 GBDT를 “속도, 성능, 안정성” 면에서 극한까지 끌어올린 진화형 모델이야. 실무에서 많이 쓰이는 이유가 다 있어!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4afb5a-2db6-4d1d-aa86-a46f55994363",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🌱 LGBMClassifier란?\n",
    "\n",
    "**LightGBM**은 Microsoft에서 만든 GBDT 기반의 머신러닝 라이브러리야.  \n",
    "핵심 목표는 **속도와 메모리 효율을 극대화하면서도 성능은 유지하거나 더 좋게** 만드는 것!\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 GBDT → XGBoost → LightGBM: 진화 흐름\n",
    "\n",
    "| 모델 | 특징 |\n",
    "|------|------|\n",
    "| GBDT | 기본 부스팅. 느리고 과적합 위험 있음 |\n",
    "| XGBoost | 빠르고 정교함. 정규화, 병렬 처리 도입 |\n",
    "| **LightGBM** | 훨씬 빠름. 대용량 데이터에 최적화. 희소/고차원 데이터도 잘 처리 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 LightGBM의 핵심 기술\n",
    "\n",
    "### 1. **Leaf-wise 성장 방식**\n",
    "- 기존 GBDT나 XGBoost는 트리를 **depth-wise**로 키워 (균형 있게).\n",
    "- LightGBM은 **leaf-wise**로 키워서 **오차를 가장 많이 줄이는 방향으로만 성장**함.\n",
    "- 결과적으로 **더 깊고 정교한 트리**가 만들어져서 성능이 좋아짐.\n",
    "\n",
    "### 2. **Histogram 기반 학습**\n",
    "- 연속형 피처를 **구간별로 묶어서(histogram)** 처리함.\n",
    "- 덕분에 **메모리 사용량이 확 줄고**, **속도도 빨라짐**.\n",
    "\n",
    "### 3. **희소 데이터 처리에 강함**\n",
    "- 텍스트 벡터처럼 **0이 많은 희소한 데이터**도 효율적으로 처리함.\n",
    "- XGBoost보다 이 부분에서 더 유리함.\n",
    "\n",
    "### 4. **GPU 지원**\n",
    "- GPU를 활용해서 대규모 데이터도 빠르게 학습 가능!\n",
    "\n",
    "---\n",
    "\n",
    "## 🏡 예시: 아파트 가격 예측\n",
    "\n",
    "- GBDT는 잔차를 줄이는 트리를 쌓아가며 예측\n",
    "- XGBoost는 병렬 처리와 정규화로 더 빠르고 안정적으로 예측\n",
    "- **LightGBM은**:\n",
    "  - 더 빠르게\n",
    "  - 더 적은 메모리로\n",
    "  - 더 정교한 트리를 만들어서\n",
    "  - **대규모 데이터에서도 훌륭한 성능**을 보여줌\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 주요 하이퍼파라미터 (기본값 기준)\n",
    "\n",
    "| 파라미터 | 설명 |\n",
    "|----------|------|\n",
    "| `num_leaves=31` | 트리에서 사용할 리프 노드 수. 많을수록 복잡한 모델 가능 |\n",
    "| `max_depth=-1` | 트리 깊이 제한 없음. 대신 num_leaves로 제어 |\n",
    "| `learning_rate=0.1` | 학습 속도. 작을수록 천천히, 과적합 방지 |\n",
    "| `n_estimators=100` | 트리 개수. 많을수록 정교하지만 느려짐 |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 장점 요약\n",
    "\n",
    "- **속도 빠름**: 특히 대용량 데이터에서 압도적\n",
    "- **메모리 효율 좋음**\n",
    "- **희소/고차원 데이터에 강함**\n",
    "- **성능 우수**: 특히 표형(tabular) 데이터에서 강력\n",
    "\n",
    "## ❌ 단점 요약\n",
    "\n",
    "- **leaf-wise 방식은 과적합 위험 있음** → 적절한 튜닝 필요\n",
    "- **파라미터 많음** → 초보자에겐 진입장벽\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 한 줄 요약\n",
    "\n",
    "> LightGBM은 “빠르고 똑똑한 GBDT”로, 대규모 데이터와 복잡한 문제를 효율적으로 해결하는 머신러닝의 실무 최강자야!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5021e42-184f-4c3d-b325-edb40eaf175b",
   "metadata": {},
   "source": [
    "| 모델         | 원리             | 장점                 | 단점                | 잘 쓰이는 곳               |\n",
    "| ---------- | -------------- | ------------------ | ----------------- | --------------------- |\n",
    "| **LogReg** | 선형 + 시그모이드     | 해석력, 빠름, 확률 잘 보정   | 비선형 못 잡음          | 텍스트, 해석 필요한 의료/사회 데이터 |\n",
    "| **SVM**    | 마진 최대화 + 커널    | 복잡한 경계 가능, 소규모에 강함 | 대규모 느림, 확률 보정 필요  | 중/소규모 비선형 분류          |\n",
    "| **RF**     | 배깅+랜덤트리        | 범용 강함, 전처리 자유로움    | 해석 어려움, 희소고차원 비효율 | 표형 데이터, 변수 중요도 분석     |\n",
    "| **GBDT**   | 부스팅            | 비선형 잘 학습, 표형 강함    | 느림, 튜닝필요          | Kaggle baseline       |\n",
    "| **XGB**    | GBDT+정규화+2차최적화 | 성능↑, 세밀튜닝          | 메모리↑, 복잡          | Kaggle SOTA           |\n",
    "| **LGBM**   | 리프 기반 GBDT     | 속도↑, 메모리 효율↑       | 소규모 불안정           | 대규모 데이터, Kaggle SOTA  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2771a18-04c9-4ced-aa74-5214e1a0e2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a4b3955-9c17-4653-b219-9d445b3c42a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nakyo\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccbbf25-073c-4903-ae69-fb83da8c91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: ['id', 'age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "cat: ['job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_unknown', 'marital_married', 'marital_single', 'education_secondary', 'education_tertiary', 'education_unknown', 'default_yes', 'housing_yes', 'loan_yes', 'contact_telephone', 'contact_unknown', 'month_aug', 'month_dec', 'month_feb', 'month_jan', 'month_jul', 'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep', 'poutcome_other', 'poutcome_success', 'poutcome_unknown']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# 0) 'unknown'을 결측치 취급\n",
    "X_train = X_train.replace(\"unknown\", np.nan)\n",
    "X_val   = X_val.replace(\"unknown\", np.nan)\n",
    "\n",
    "# 0.5) bool/boolean 컬럼을 object(문자열)로 바꾸거나, 0/1 숫자로 바꾼다 (둘 중 하나 택1)\n",
    "\n",
    "# [옵션 A] 카테고리로 처리하고 싶다 → object로 캐스팅\n",
    "bool_cols = X_train.select_dtypes(include=[\"bool\", \"boolean\"]).columns.tolist()\n",
    "if bool_cols:\n",
    "    X_train[bool_cols] = X_train[bool_cols].astype(\"object\")\n",
    "    X_val[bool_cols]   = X_val[bool_cols].astype(\"object\")\n",
    "\n",
    "# [옵션 B] 숫자(0/1)로 처리하고 싶다 → int8로 캐스팅 (위 A 대신 이걸 쓰려면 A는 주석 처리)\n",
    "# bool_cols = X_train.select_dtypes(include=[\"bool\", \"boolean\"]).columns.tolist()\n",
    "# if bool_cols:\n",
    "#     X_train[bool_cols] = X_train[bool_cols].astype(\"int8\")\n",
    "#     X_val[bool_cols]   = X_val[bool_cols].astype(\"int8\")\n",
    "\n",
    "# 1) 컬럼 타입 분리 (캐스팅 후에 다시 뽑기 중요!)\n",
    "num_cols  = X_train.select_dtypes(include=[\"int64\", \"float64\", \"int32\", \"float32\", \"int16\", \"float16\", \"int8\"]).columns.tolist()\n",
    "cat_cols  = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"num:\", num_cols)\n",
    "print(\"cat:\", cat_cols)\n",
    "\n",
    "# 2) 파이프라인\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  StandardScaler()),\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258b893-f064-499c-9bdf-57388c020417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML 모델 비교 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 학습 진행중:  17%|███████████                                                       | 1/6 [00:25<02:09, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report \n",
    "\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=2000, class_weight=\"balanced\"),\n",
    "    # 선형 모델 z = w*x + b를 sigmoid σ(z)로 변환해서 P(y=1|x) 확률을 출력 / 다중 분류는 softmax\n",
    "    # max_iter=2000 → 수렴할 때까지 경사하강법 반복 횟수 충분히 늘림.\n",
    "    # class_weight=\"balanced\" → 클래스 비율이 불균형할 때 자동으로 가중치 조정.\n",
    "    # 장점: 빠름, 해석력(가중치 방향/크기) 좋음, 확률 출력이 안정적.\n",
    "    # 단점: 선형 경계만 학습 → 비선형 패턴은 잘 못 잡음. 스케일링 필수.\n",
    "    \"svm\": SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\"),\n",
    "    # 원리: “마진”을 최대화하는 결정경계를 찾음. 비선형 패턴은 커널(여기선 RBF)로 변환해 분리.\n",
    "    # = 데이터를 가장 잘 분류하면서도 클래스 간의 여유 공간을 최대한 확보하는 경계선을 찾는다\n",
    "    # kernel=\"rbf\" → 가우시안(RBFRadial Basis Function) 커널로 복잡한 곡선형 경계 표현 가능.\n",
    "    # probability=True → Platt scaling을 이용해 확률 값까지 출력 (추가 연산 있음).\n",
    "    # class_weight=\"balanced\" → 불균형 클래스 대응.\n",
    "    # 장점: 중/소규모 데이터에서 비선형 분류 잘함. 마진 극대화라 일반화 성능이 좋음.\n",
    "    # 단점: 데이터 크면 학습/예측 느려짐. 파라미터(C, gamma) 튜닝 필수.\n",
    "    \"rf\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    # 원리: 배깅(Bagging) 기반 앙상블. 부트스트랩 샘플 + 랜덤 피처 선택으로 여러 트리 학습 → 투표.\n",
    "    # n_estimators=400 → 트리 400개. 많을수록 안정성↑, 시간/메모리 비용도 ↑.\n",
    "    # n_jobs=-1 → 멀티코어 병렬 처리.\n",
    "    # 장점: 전처리 거의 필요 없음, 범용성 높음, 과적합에 강함.\n",
    "    # 단점: 해석력 낮음, 매우 희소/고차원 데이터엔 비효율.\n",
    "    \"gbdt\": GradientBoostingClassifier(random_state=42),\n",
    "    # 원리: 부스팅(Boosting). 이전 트리의 오차(잔차)를 점점 줄여가며 약한 트리들을 순차적으로 쌓음.\n",
    "    # 하이퍼파라미터 (기본값):\n",
    "    # learning_rate=0.1 (작을수록 천천히, 과적합 방지)\n",
    "    # n_estimators=100 (트리 개수, 많을수록 정교)\n",
    "    # max_depth=3 (얕은 트리로 단계적 보정)\n",
    "    # 장점: 복잡한 비선형 패턴을 잘 잡음. 표형(tabular)에서 매우 강력.\n",
    "    # 단점: 파라미터 많음, 학습 느림, 과적합 주의 → 보통 XGBoost/LightGBM으로 대체.\n",
    "    \"xgb\": XGBClassifier(\n",
    "        n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.9, colsample_bytree=0.9, eval_metric=\"logloss\", \n",
    "        tree_method=\"gpu_hist\",\n",
    "        predictor=\"gpu_predictor\"\n",
    "    ),\n",
    "    # 원리: GBDT 발전형. 2차 도함수까지 사용한 정교한 최적화 + 정규화로 과적합 방지.\n",
    "    # 주요 파라미터:\n",
    "    # n_estimators=600 → 트리 수.\n",
    "    # max_depth=6 → 개별 트리 최대 깊이. 깊을수록 복잡 패턴, 과적합 위험↑.\n",
    "    # learning_rate=0.05 → 작은 학습률로 천천히 학습 (트리 개수는 늘려야 함).\n",
    "    # subsample=0.9 → 샘플 일부만 사용해 과적합 방지.\n",
    "    # colsample_bytree=0.9 → 피처 일부만 사용해 트리 학습.\n",
    "    # tree_method=\"hist\" → 히스토그램 기반 빠른 분할 (GPU 쓰려면 \"gpu_hist\").\n",
    "    # 장점: 속도/성능 균형 최고, 파라미터 세밀 조정 가능.\n",
    "    # 단점: 튜닝 필요, 메모리 사용량 ↑.\n",
    "    \"lgbm\": LGBMClassifier(\n",
    "        n_estimators=1000, learning_rate=0.05,\n",
    "        subsample=0.9, colsample_bytree=0.9, objective=\"binary\",\n",
    "        device=\"gpu\"\n",
    "    )\n",
    "    # (LightGBM)\n",
    "    # 원리: 또 다른 GBDT 구현. 리프 중심(Tree leaf-wise) 성장 + 히스토그램 기반 분할로 속도와 메모리 효율 뛰어남.\n",
    "    # 주요 파라미터:\n",
    "    # n_estimators=1000 → 트리 개수.\n",
    "    # learning_rate=0.05 → 낮은 학습률, 과적합 방지.\n",
    "    # subsample=0.9, colsample_bytree=0.9 → 데이터/특징 서브샘플링.\n",
    "    # objective=\"binary\" → 이진 분류.\n",
    "    # 장점: 대규모 데이터에서 빠르고 메모리 효율적. 카테고리형 직접 처리 가능(categorical_feature).\n",
    "    # 단점: 작은 데이터에서 오히려 불안정할 수 있음. 카테고리 처리 잘못 쓰면 성능 저하.\n",
    "}\n",
    "\n",
    "print(\"\\n=== ML 모델 비교 ===\")\n",
    "ml_results = []\n",
    "for name, est in tqdm(models.items(), desc=\"모델 학습 진행중\", total=len(models)):\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"est\", est)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    try: \n",
    "        y_prob = pipe.predict_proba(X_val)[:, 1]\n",
    "    except Exception:\n",
    "        y_prob = None\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1m = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    auc = (roc_auc_score(y_val, y_prob) if y_prob is not None else np.nan)\n",
    "    ml_results.append((name, acc, f1m, auc))\n",
    "    print(f\"{name:>6s}\")\n",
    "    # >: 오른쪽 정렬, 6: 총 너비 6칸, s: - 문자열(string) 타입 (참고: ^6s이면 가운데 정렬임)\n",
    "\n",
    "best_name, *_ = sorted(ml_results, key=lambda t: (np.nan_to_num(t[3]), t[2]), reverse=True)[0]\n",
    "# t[3]: AUC 점수, t[2]: F1 macro 점수 \n",
    "# - AUC가 np.nan일 수도 있으니까, 그걸 0.0으로 바꿔서 비교 가능하게 만들어줌.\n",
    "# - 즉, AUC가 없는 모델은 성능이 낮다고 간주함.\n",
    "# - AUC를 우선 기준으로, F1 macro를 보조 기준으로 정렬함.\n",
    "# [0]: - 정렬된 리스트에서 가장 성능이 좋은 모델 하나만 선택\n",
    "# - 나머지 값들(acc, f1m, auc)은 _로 무시\n",
    "print(f\"\\nBest ML model (AUC/F1 기준): {best_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4d5e39-b4c3-49ae-9e47-9a483bf5a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def is_binary(y):\n",
    "    u = np.unique(y)\n",
    "    return len(u) == 2 and set(u) <= {0, 1}\n",
    "\n",
    "def get_scores_for_auc(pipe, X):\n",
    "    # predict_proba 있으면 1클래스 확률, 없으면 decision_function 사용\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        p = pipe.predict_proba(X)\n",
    "        if p.ndim == 2 and p.shape[1] >= 2:\n",
    "            return p[:, 1]\n",
    "    if hasattr(pipe, \"decision_function\"):\n",
    "        return pipe.decision_function(X)\n",
    "    return None\n",
    "\n",
    "def evaluate_model(estimator, name):\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"est\", estimator)])\n",
    "    t0 = time.time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    fit_sec = time.time() - t0\n",
    "\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1m = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    scores = get_scores_for_auc(pipe, X_val)\n",
    "    auc = roc_auc_score(y_val, scores) if (scores is not None and is_binary(y_val)) else np.nan\n",
    "\n",
    "    print(f\"[{name}] acc={acc:.4f} | f1m={f1m:.4f} | auc={auc if not np.isnan(auc) else float('nan'):.4f} | time={fit_sec:.1f}s\")\n",
    "    return {\"name\": name, \"acc\": acc, \"f1m\": f1m, \"auc\": auc, \"time\": fit_sec}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e15312-0c3d-42d8-b7b0-b545583ebeb9",
   "metadata": {},
   "source": [
    "# PyTorch MLP 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4831d3-d390-4623-ac6b-8e6126170e73",
   "metadata": {},
   "source": [
    "# 🧩 희소 행렬(Sparse Matrix) vs 밀집 배열 (Dense Array)\n",
    "Sparse Matrix: 대부분의 값이 0인 행렬. 메모리 절약을 위해 0이 아닌 값만 저장함.\\\n",
    "Dense Array: 모든 값을 다 저장하는 일반적인 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd85425-6a7e-43cf-8019-62ad2ba8717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline as Skpipe\n",
    "pre_only = Skpipe([(\"pre\", pre)])\n",
    "Xtr_t = pre_only.fit_transform(X_train)\n",
    "# 데이터를 보고 학습(fit) + 변환(transform)을 동시에 수행\n",
    "Xva_t = pre_only.transform(X_val)\n",
    "# 이미 학습된 전처리기를 사용해서 변환만 수행\n",
    "\n",
    "# 희소 행렬(sparse matrix) -> 밀집 배열(dense array)로 변환 \n",
    "try:\n",
    "    import scipy.sparse as sp\n",
    "    if sp.issparse(Xtr_t): Xtr_t = Xtr_t.toarray()\n",
    "    if sp.issparse(Xva_t): Xva_t = Xva_t.toarray()\n",
    "except Exception:\n",
    "    Xtr_t = np.asarray(Xtr_t); Xva_t = np.asarray(Xva_t)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# TensorDataset: 여러 개의 tensor들을 하나의 데이터셋 객체로 묶어줌 _ - 예: 입력 데이터와 정답(label)을 한 쌍으로 만들어서 모델이 학습할 수 있게 함.\n",
    "# DataLoader: TensorDataset같은 데이터셋을 batch 단위로 나눠서 모델에 공급해주는 역할 \n",
    "# -> 학습 시 미니배치 학습, 셔플, 병렬 처리 등을 쉽게 할 수 있음 \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Xtr_tensor = torch.tensor(Xtr_t, dtype=torch.float32)\n",
    "Xva_tensor = torch.tensor(Xva_t, dtype=torch.float32)\n",
    "ytr_tensor = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "yva_tensor = torch.tensor(np.array(y_val), dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr_tensor, ytr_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(Xva_tensor, yva_tensor), batch_size=512, shuffle=False)\n",
    "\n",
    "in_dim = Xtr_t.shape[1]\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.15),\n",
    "    nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.15),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# 불균형 대응: pos_weight = neg/pos\n",
    "pos = (y_train==1).sum()\n",
    "neg = (y_train==0).sum()\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float, device=device)\n",
    "# 혹시나 pos가 0일 경우 나눗셈 오류 방지 \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "# Binary Cross Entropy Loss + Sigmoid를 합친 손실 함수 _ 이진 분류에서 자주 사용됨 \n",
    "# sigmoid를 먼저 적용한 뒤 binary cross entropy 계산 \n",
    "# pos weight -> for 불균형 해결 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# AdamW는 Adam optimizer의 변형이고, Weight Decay를 더 제대로 처리해주는 버전 \n",
    "# 일반 Adam은 L2 정규화를 잘못 적용하는 문제가 있었는데 AdamW는 그걸 고침 \n",
    "# model.parameters(): 모델의 학습 가능한 파라미터들을 가져옴 (가중치, 편향 등)\n",
    "# lr=1e-3: 학습률을 0.001로 설정. 이 값은 파라미터를 얼마나 빠르게 업데이트할지를 결정\n",
    "# AdamW: parameter update 방식으로 사용 _ 내부적으로는 momentum, adaptice learning rate, weight decay를 잘 조합함 \n",
    "# Adam: (특징) 빠른 수렴, 적응적 학습률 (장점) 일반적인 상황에서 잘 작동\n",
    "# AdamW: (특징) Weight Decay 분리 적용, (장점) 정규화가 필요한 모델에 더 안정적 \n",
    "\n",
    "best_auc = -1.0\n",
    "best_state = None\n",
    "for epoch in range(1,21): \n",
    "    model.train()\n",
    "    # 모델을 학습 모드로 설정 \n",
    "    epoch_loss = 0.0\n",
    "    # 한 epoch 동안의 총 손실을 저장할 변수 \n",
    "    for xb, yb in train_loader:\n",
    "        # train_loader는 DataLoader로부터 batch 단위로 데이터를 가져옴 \n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        # xb: 입력 데이터, yb: 정답 라벨 \n",
    "        optimizer.zero_grad()\n",
    "        # 이전 배치에서 계산된 graedient 초기화 \n",
    "        logits = model(xb).squeeze(1)\n",
    "        # 모델에 입력을 넣고 예측값(logits)을 얻음\n",
    "        # squeeze(1)은 [batch_size, 1] -> [batch_size]로 차원 축소\n",
    "        loss = criterion(logits, yb.float())\n",
    "        # 예측값과 실제값을 비교해서 손실 계산 \n",
    "        loss.backward()\n",
    "        # 손실을 기준으로 gradient 계산 (역전파)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # gradient 폭주 방지용. gradient의 norm을 최대 1.0으로 제한 \n",
    "        optimizer.step()\n",
    "        # 계산된 gradient 기반으로 parameter update \n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "        # loss.item: 현재 배치의 평균 손실 \n",
    "        # 배치 평균 손실 × 배치 크기 = 배치 전체 손실 합\n",
    "        # 현재 배치의 손실 값을 float로 가져옴\n",
    "        # 이걸 epoch_loss에 계속 더해서 → 한 epoch 전체 손실 합을 구하는 거야\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    # 전체 손실을 샘플 수 기준으로 누적해서 평균 손실 계산 \n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    probs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        # gradient 게산을 끔 -> 메모리 절약 + 속도 향상 \n",
    "        # validation은 학습이 아니니까 gradient 필요 없음 \n",
    "        for xb, yb in val_loader: # validation 데이터를 batch 단위로 불러옴 \n",
    "            xb = xb.to(deivice)\n",
    "            p = torch.sigmoid(model(xb).squeeze(1)).cpu().numpy()\n",
    "            # 모델에 입력 넣고 logits 출력, 출력이 [batch_size, 1]일 경우 → squeeze(1)로 [batch_size]로 바꿔줌\n",
    "            # logits를 확률로 변환 \n",
    "            # sigmoi는 0~1 사이의 갑승로 바꿔주니까 이게 양성 클래스일 확률로 나옴\n",
    "            probs.append(p); ys.append(yb.numpy())\n",
    "            # p는 numpy 배열로 변환된 확률값 \n",
    "            # 이걸 리스트에 계속 누적해서 전체 validation 결과 저장  \n",
    "        probs = np.concatenate(probs); ys = np.concatenate(ys)\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "        f1m = f1_score(ys, preds, average=\"macro\")\n",
    "        try: \n",
    "            auc = roc_auc_score(ys, probs)\n",
    "        except ValueError: \n",
    "            auc = np.nan\n",
    "\n",
    "        if (auc if not np.isnan(auc) else -1) > best_auc: \n",
    "            # AUC가 이전보다 더 좋으면 모델 상태 저장 \n",
    "            best_auc = auc \n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            # 모델의 모든 파라미터(가중치, 편향 등)를 딕셔너리 형태로 반환 \n",
    "            # GPU에 있던 tensor를 CPU로 옮기고 복사함 _ 왜 복사하냐면 이후 학습이 계속되면 모델 파라미터가 바뀌니까, 그 순간의 상태를 안전하게 저장하려고  \n",
    "        \n",
    "        print(f\"[EP {epoch:02d}] train_loss={epoch_loss:.4f} val_f1={f1m:.4f} val_auc={auc:.4f}\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "print(f\"Best DL AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eeb0f3-7f80-4e24-84f3-1170b4ee7fda",
   "metadata": {},
   "source": [
    "# 데이터 형태 정확하게 맞춰주기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dcad8-be4f-480f-b667-7f147cf0b173",
   "metadata": {},
   "source": [
    "PyTorch에서는 tensor와 numpy array 사이의 변환이 자주 일어남 _ 정리 필요 \n",
    "\n",
    "### 🔁 tensor vs numpy array 차이점\n",
    "| 항목            | torch.Tensor                  | numpy.ndarray                  |\n",
    "|-----------------|-------------------------------|--------------------------------|\n",
    "| 사용 목적       | PyTorch 모델 학습, 연산       | 일반적인 수치 계산, 평가        |\n",
    "| GPU 사용 가능   | ✅ 가능 (`.to(device)`)       | ❌ 불가능                       |\n",
    "| 자동 미분       | ✅ 가능 (`.backward()`)       | ❌ 불가능                       |\n",
    "| 모델 입력       | ✅ 사용됨                     | ❌ 직접 입력 불가               |\n",
    "| 평가 지표 계산  | ❌ 불편함                     | ✅ `sklearn` 등에서 사용 용이   |\n",
    "\n",
    "\n",
    "### 📥 모델에 들어갈 때: torch.Tensor\n",
    "- 모델에 입력할 때는 무조건 tensor 형태여야 해\n",
    "- 예: model(xb)에서 xb는 tensor여야 함\n",
    "- GPU에서 학습하려면 .to(device)도 꼭 해줘야 함\\\n",
    ">xb = xb.to(device)  # tensor 형태로 GPU에 올림\\\n",
    "logits = model(xb)  # 모델에 입력\n",
    "\n",
    "\n",
    "\n",
    "### 📤 모델에서 나올 때: tensor → numpy로 변환\n",
    "- 모델 출력은 tensor 형태야\n",
    "- 평가할 때는 numpy로 바꿔서 sklearn 같은 라이브러리에 넘겨줘야 함\n",
    ">p = torch.sigmoid(logits).cpu().numpy()  # 확률로 바꾸고 numpy로 변환\n",
    "\n",
    "\n",
    "- .cpu()는 GPU에서 CPU로 옮기는 작업\n",
    "- .numpy()는 tensor → numpy array로 변환\n",
    "\n",
    "### 🎯 왜 변환하냐면...\n",
    "- PyTorch는 학습에 최적화된 프레임워크\n",
    "- numpy는 평가 지표 계산, 시각화 등에 더 적합\n",
    "- 예: roc_auc_score, confusion_matrix, plot() 등은 numpy를 요구함\n",
    "\n",
    "### 💡 흐름 요약\n",
    "- 데이터 로딩: numpy → tensor로 변환 (TensorDataset)\n",
    "- 모델 학습: tensor 형태로 GPU에 올려서 학습\n",
    "- 모델 출력: tensor 형태로 나옴\n",
    "- 평가 단계: tensor → numpy로 변환해서 지표 계산\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708a89a-64ec-419f-8a9c-c8abd1230075",
   "metadata": {},
   "source": [
    "# 머신러닝 데이터 형태 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4b2e3-b3aa-44c9-ab64-5c4ffc8b2fda",
   "metadata": {},
   "source": [
    "대부분의 머신러닝 library(ex: scikit-learn)은 numpy.ndarray 또는 pandas.DataFrame 형태의 데이터 사용함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e396b-4748-4d5b-ad7f-3c712cd18d59",
   "metadata": {},
   "source": [
    "# numpy.ndarray vs torch.Tensor\n",
    "> np_arr = np.array([[1, 2], [3, 4]])       # shape: (2, 2)\\\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])   # shape: (2, 2)\n",
    "\n",
    "> **numpy 연산**\\\n",
    ">np_arr = np.array([[1, 2], [3, 4]])\n",
    ">print(np_arr * 2)  # 단순 곱셈\n",
    "\n",
    "> **tensor 연산**\\\n",
    ">tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "print(tensor * 2)  # 곱셈 + gradient 추적 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee56ad0-1e93-46d8-9430-342fa8199eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
